{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_TeFobPMflU"
   },
   "source": [
    "# **Text Mining | <span style=\"color: khaki;\">Project Notebook</span>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQGI5rQ8MflX"
   },
   "source": [
    "#### NOVA IMS / BSc in Data Science / Text Mining 2024/2025\n",
    "### <b>Group Project: Solving the Hyderabadi Word Soup</b>\n",
    "#### Notebook `Multilabel Classification & Sentiment Analysis & Topic Modelling & Co-occurrence Analysis`\n",
    "\n",
    "#### Group:\n",
    "- `Francisco Gomes (20221810)`\n",
    "- `Maria Henriques (20221952)`\n",
    "- `Carolina Almeida (20221855)`\n",
    "- `Duarte Carvalho (20221900)`\n",
    "- `Marta Monteiro (20221954)`\n",
    "\n",
    "#### <font color='#BFD72'>Table of Contents </font> <a class=\"anchor\" id='toc'></a>\n",
    "- [1. Data Understanding](#P1)\n",
    "- [2. General Data Preparation](#P2)\n",
    "- [3. Multilabel Classification](#P3)\n",
    "    - [3.1 Specific Data Preparation](#P31)\n",
    "    - [3.2 Model Implementation](#P32)\n",
    "    - [3.3 Model Evaluation](#P3n)\n",
    "- [4. Sentiment Analysis](#P4)\n",
    "    - [4.1 Specific Data Preparation](#P41)\n",
    "    - [4.2 Model Implementation](#P42)\n",
    "    - [4.3 Model Evaluation](#P43)\n",
    "- [5. Topic Modelling](#P5)\n",
    "- [6. Co-occurrence Analysis](#P6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>1. Data Understanding</font> <a class=\"anchor\" id=\"P1\"></a>\n",
    "  \n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Importing the Datasets and Libraries</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section, as mentioned in the title, is intended for importing the data and necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1729785884869,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "huYQC7IeMflY",
    "outputId": "d71c4288-cc9d-4fa0-b847-b514bc110b69"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Import Essential Libraries\n",
    "# ==============================\n",
    "\n",
    "# General-purpose libraries\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import emoji\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable, HRuleStyle, VRuleStyle\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing and NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from num2words import num2words\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Statistical and correlation analysis\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer, classification_report, mean_squared_error, mean_absolute_error\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Import TextBlob for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Network graph libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities \n",
    "from adjustText import adjust_text\n",
    "import matplotlib.cm as cm \n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.filterwarnings(\"ignore\", category = RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = SettingWithCopyWarning)\n",
    "\n",
    "# ==============================\n",
    "# Load Project-specific Modules\n",
    "# ==============================\n",
    "from Functions_Group_8 import *\n",
    "\n",
    "# ==============================\n",
    "# Configure Notebook Settings\n",
    "# ==============================\n",
    "\n",
    "# Enable autoreload for custom modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the respective paths for the group members. \n",
    "<br>\n",
    "Please add any new paths as needed to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1729784751087,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "-BAMzrnTMfla"
   },
   "outputs": [],
   "source": [
    "# path = '/Users/franciscogomes/Desktop/Faculdade/3rd year/1st Semester/Text Mining/Project TM08/Project-Text-Mining-main/data_hyderabad'\n",
    "path = '/Users/carol/Desktop/NOVA IMS/Third Year - First Semester/Text Mining/Group Project/Group 8/Text_Mining_Group_8/Data'\n",
    "# path = 'C:/Users/marga/OneDrive/Documentos/universidade/3º ano/1º semestre/Text Mining/Text Mining - Project/data_hyderabad'\n",
    "# path = 'C:/Users/dacar/OneDrive - NOVAIMS/Ambiente de Trabalho/Text Mining Project/data_hyderabad'\n",
    "# path = '\\Users\\marta\\Desktop\\eu\\faculdade\\3\\First Semester\\Text Mining\\Projeto\\Project Statement\\data_hyderabad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "error",
     "timestamp": 1729784751807,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "D3kG4K5LMfla",
    "outputId": "557f5121-5eb5-4727-8c51-09a6b08c8b08"
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(path + '/10k_reviews.csv')\n",
    "restaurants = pd.read_csv(path + '/105_restaurants.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Preliminary Data Analysis and Arrangements</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we extracted some initial information regarding data types, missing values and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `deepcopy` on both DataFrames to ensure that any modifications made to these copies do not affect the original DataFrames, preserving their integrity for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = deepcopy(reviews)\n",
    "restaurants = deepcopy(restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following outputs show the structure and content of the <b>Reviews</b> and <b>Restaurants</b> DataFrames, confirming that the data has been successfully duplicated and is ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1729785890451,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "b_xTAS_uMflb",
    "outputId": "a2063d8e-4657-4fe4-91bd-bb3369265f6c"
   },
   "outputs": [],
   "source": [
    "display(reviews.head())\n",
    "display(restaurants.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below helps us examine missing values, data types, and memory usage of the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1729785891790,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "yR0KGta8Mflb",
    "outputId": "507cd336-df22-4c6d-9fc3-71726a39fda3"
   },
   "outputs": [],
   "source": [
    "display(reviews.info(memory_usage = 'deep'))\n",
    "display(restaurants.info(memory_usage = 'deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick glance at the <b>Reviews</b> dataset reveals that the variables <u>Reviewer</u>, <u>Review</u>, <u>Rating</u>, <u>Metadata</u>, and <u>Time</u> contain missing values. \n",
    "<br>\n",
    "In the <b>Restaurant</b> dataset, only the <u>Collections</u> variable has missing values. \n",
    "\n",
    "For the first dataset, the missing values will be dropped as they constitute an insignificant proportion. \n",
    "<br>\n",
    "However, for the second dataset, the missing values in the collections column will remain untouched, as this variable will not be used in the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we decided to change the data type of the variable <u>Time</u> to datetime, as it would be more suitable for future manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Year'] = pd.to_datetime(reviews['Time'], format = '%m/%d/%Y %H:%M').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Data Exploration and Treatment</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to extract useful data insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5yzQyxVMflc"
   },
   "source": [
    "<u>Reviews Dataset</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are some insights that we can take?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Restaurant:** There are 10000 entries, with 100 unique restaurant names. The most frequently reviewed restaurant is \"Beyond Flavours,\" appearing 100 times.\n",
    "- **Reviewer:** Among the 9962 entries, there are 7446 unique reviewers. The most active reviewer is \"Parijat Ray,\" contributing 13 reviews.\n",
    "- **Review:** Of the 9955 reviews, 9364 are unique. The most commonly occurring review is the word \"good,\" appearing 237 times, suggesting it might be a frequent descriptor.\n",
    "- **Rating:** There are 9962 ratings spanning 10 unique values, with the highest frequency being 3832 for a rating of 5, indicating a strong positive bias in ratings.\n",
    "- **Metadata:** Metadata is present in 9962 entries, with 2477 unique values. The most common metadata entry is \"1 Review,\" appearing 919 times, possibly indicating a majority of single-review users.\n",
    "- **Time:** The time column includes 9962 entries with 9782 unique values. The most frequent timestamp is \"7/29/2018 20:34,\" appearing 3 times, which might suggest either duplicate entries or coincidental timing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code checks for inconsistent values in the <u>Rating</u> column that fall outside the valid range of 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1729785893295,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "8gja2aZ7Mflc",
    "outputId": "7d85c9ee-c27d-47e9-f263-9cdead8d9f10"
   },
   "outputs": [],
   "source": [
    "reviews['Rating'] = pd.to_numeric(reviews['Rating'], errors = 'coerce')\n",
    "\n",
    "out_of_range_ratings = reviews[(reviews['Rating'] < 1) | (reviews['Rating'] > 5)]\n",
    "\n",
    "# Display the out-of-range ratings\n",
    "if not out_of_range_ratings.empty:\n",
    "    print(\"Values in 'Rating' that are out of range (not between 1 and 5):\")\n",
    "    print(out_of_range_ratings)\n",
    "else:\n",
    "    print(\"All values in 'Rating' are within the range of 1 to 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code removes rows from the **Reviews** DataFrame where all values in the specified columns are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1729785895368,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "dULRR46AMflc"
   },
   "outputs": [],
   "source": [
    "reviews.dropna(how = 'all', subset = ['Reviewer', 'Review', 'Rating', 'Metadata', 'Time'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review'] = reviews['Review'].fillna('No Review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code identifies and counts the total number of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729785895963,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "mlnnyvGYMfld",
    "outputId": "8ee1d303-0e72-42e8-b135-d7411e969bd2"
   },
   "outputs": [],
   "source": [
    "reviews[reviews.duplicated()].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate entries, eliminating the need to address duplicate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a quick insight into the distribution of our reviews, we implemented a simple code that categorizes reviews into positive, neutral, and negative groups. \n",
    "<br>\n",
    "A more detailed analysis will be provided in `4. Sentiment Analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_ = reviews.groupby('Review').size().to_frame().reset_index().rename(columns = {0: 'Count'})\n",
    "\n",
    "positive_words = ['good', 'very good', 'excellent', 'great', 'nice', 'best', 'amazing', 'superlative']\n",
    "negative_words = ['bad', 'worst', 'terrible', 'horrible', 'poor', 'disappointing', 'late', 'disappointed']\n",
    "\n",
    "def categorize_review(review):\n",
    "    if any(word in review.lower() for word in positive_words):\n",
    "        return 'Positive'\n",
    "    elif any(word in review.lower() for word in negative_words):\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "Reviews_['Category'] = Reviews_['Review'].apply(categorize_review)\n",
    "\n",
    "Reviews_[Reviews_['Count'] > 1].sort_values(by = 'Count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(Reviews_, variable = 'Category', x_label = 'Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the most frequent reviews are positive, followed by neutral, and then negative.\n",
    "<br>\n",
    "While this is a basic approach and doesn't capture all nuances of the reviews, we expect the overall distribution to remain similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this exploration, we also noticed the presence of the word **nic**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[reviews['Review'].str.contains(r'\\bnic\\b', case = False, na = False, regex = True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that this word appears in two rows, both associated with a rating of 5, which suggests it was intended to be **nice**. \n",
    "<br>\n",
    "Therefore, we will address these two cases. \n",
    "\n",
    "While more thorough preprocessing of the review text will be done later, we chose not to ignore these instances for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.loc[3795, 'Review'] = re.sub(r'\\bnic\\b', 'nice', reviews.loc[3795, 'Review'], flags = re.IGNORECASE)\n",
    "reviews.loc[7574, 'Review'] = re.sub(r'\\bnic\\b', 'nice', reviews.loc[7574, 'Review'], flags = re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9Pls8FyMfld"
   },
   "source": [
    "<u>Restaurants Dataset</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are some insights that we can take?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Name:** There are 105 entries, each corresponding to a unique restaurant name.\n",
    "- **Links:** Each restaurant has a unique link.\n",
    "- **Cost:** This column contains 105 entries, with 29 unique values. The most common cost is 500, appearing 13 times.\n",
    "- **Collections:** It has 51 entries, with 42 unique values. The most frequent collection is \"Food Hygiene Rated Restaurants in Hyderabad,\" which appears 4 times.\n",
    "- **Cuisines:** There are 105 entries in ths column, with 92 unique values. The most common cuisine combination is \"North Indian, Chinese,\" appearing 4 times.\n",
    "- **Timings:** The \"Timings\" column contains 104 entries, with 77 unique values. The most frequent timing is \"11 AM to 11 PM,\" appearing 6 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code checks for inconsistent values in the <u>Cost</u> column that fall outside the positive range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1729785897356,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "gmA4dl5jMfld",
    "outputId": "632b899c-6d49-4ac2-9f11-b6e457c575bd"
   },
   "outputs": [],
   "source": [
    "restaurants['Cost'] = pd.to_numeric(restaurants['Cost'], errors = 'coerce')\n",
    "\n",
    "negative_costs = restaurants[restaurants['Cost'] < 0]\n",
    "\n",
    "if not negative_costs.empty:\n",
    "    print(\"Negative values in 'Cost':\")\n",
    "    print(negative_costs)\n",
    "else:\n",
    "    print(\"No negative values in 'Cost'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code identifies and counts the total number of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1729785899511,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "h1Ac2FHVMfld",
    "outputId": "0efc7772-221d-4d30-82d1-7dd3aaee781e"
   },
   "outputs": [],
   "source": [
    "restaurants[restaurants.duplicated()].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate entries, eliminating the need to address duplicate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX9L9pHBMfle"
   },
   "source": [
    "## <b>Data Visualization<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explored the data and created visualizations to draw some initial conclusions and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Ratings in Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1729785902474,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "5WDiMgZeMfle",
    "outputId": "54baab31-5ce9-4633-e9df-81c1747f81c7"
   },
   "outputs": [],
   "source": [
    "bar_plot(reviews, variable = 'Rating', x_label = 'Rating', y_label = 'Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that 3832 reviews have a rating of 5, followed by 2373 with a rating of 4, 1193 with a rating of 3, 684 with a rating of 2, and 1735 with a rating of 1. \n",
    "<br>\n",
    "This indicates that, on average, there are more positive ratings than negative ones, as already mentioned above. \n",
    "<br>\n",
    "Additionally, we note the presence of some decimal ratings, reflecting indecisiveness among a few reviewers. However, these cases represent a small minority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Reviews with Pictures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729785902474,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "wOAkLI-LMfle",
    "outputId": "c17d97ac-cd84-4477-9b6c-fa39d3ae83b2"
   },
   "outputs": [],
   "source": [
    "bar_plot(reviews, variable = 'Pictures', x_label = 'Pictures posted with review', y_label = 'Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of reviews were submitted without accompanying pictures, although some reviews do include images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Most Frequent Words in Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1729785930321,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "7pBObI3bMfle",
    "outputId": "769b76bb-bfce-4fcf-9185-27acf3f1903b"
   },
   "outputs": [],
   "source": [
    "donut_chart(top_words(reviews, 'Review', top_n = 10), title = 'Top 10 Words in the column Review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>most frequently used word</b> in the <b>Reviews</b> is <u>the</u>, followed by <u>and</u> and <u>was</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 20 Most Frequent Words in Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1729785934693,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "3J_0qzPvMfle",
    "outputId": "56de75ca-3e97-4781-de6b-72efed11b434"
   },
   "outputs": [],
   "source": [
    "word_cloud_1 = word_cloud(top_words(reviews, column_name = 'Review').set_index('Word')['Frequency'].to_dict())\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.imshow(word_cloud_1, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Top 20 Words in the column Review', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word cloud conveys the same insights as the previous donut chart but presents them in a more visually engaging manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to highlight that the most frequent words in the reviews are predominantly stop words. \n",
    "<br>\n",
    "These words will be removed during the <font color='#BFD72F' size=3>2. General Data Preparation</font> <a class=\"anchor\" id=\"P2\"></a> stage, as they hold no significance for the final multilabel classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Reviews per Year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_reviews_per_year = reviews.groupby('Year').size().reset_index(name = 'Review Count')\n",
    "line_plot(number_of_reviews_per_year, 'Year', 'Review Count', 'Number of Reviews per Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2016, there were only 43 reviews, with a slight increase in 2017. \n",
    "<br>\n",
    "However, the most significant growth occurred between 2017 and 2018, when the number of reviews surged by over 4000. \n",
    "<br>\n",
    "The peak was reached in 2018, with 4903 reviews, followed by a slight decrease in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Popularity of Restaurants Mentioned in Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_2 = word_cloud(reviews['Restaurant'].value_counts())\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.imshow(word_cloud_2, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Popularity of Restaurants in Reviews', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that <u>Beyond Flavours</u>, <u>Driven Cafe</u>, and <u>Eat India Company</u> are the most reviewed restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Popularity of Cuisine Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cuisines = [cuisine.strip() for cuisine in restaurants['Cuisines'].dropna().str.split(',').sum()]\n",
    "word_cloud_3 = word_cloud(pd.Series(list_of_cuisines).value_counts())\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.imshow(word_cloud_3, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Popularity of Cuisine Types', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>North Indian</u> is the type of cuisine with the most reviews.\n",
    "<br>\n",
    "This likely suggests that the restaurants mentioned above specialize in this type of cuisine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWp9YVbvMflf"
   },
   "source": [
    "<font color='#BFD72F' size=5>2. General Data Preparation</font> <a class=\"anchor\" id=\"P2\"></a>\n",
    "  \n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Preprocessing Text</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we aim to clean the <b>Reviews</b> column, as it contains many irrelevant elements that could negatively impact the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by integrating the two datasets (<i>reviews</i> and <i>restaurants</i>) and removing irrelevant columns. \n",
    "<br>\n",
    "The resulting <b>integrated_datasets</b> is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1729785937277,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "cgxgZrBkMflf",
    "outputId": "f9485b27-6623-42e1-e72f-a47e567efa38"
   },
   "outputs": [],
   "source": [
    "integrated_datasets = pd.merge(restaurants, reviews, left_on = 'Name', right_on = 'Restaurant')\n",
    "integrated_datasets.drop(['Collections', 'Reviewer','Name', 'Pictures','Links', 'Metadata', 'Cost', 'Timings', 'Time', 'Year'], axis = 1, inplace = True)\n",
    "display(integrated_datasets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, considering that we have two phases in this project, each requiring different types of text, we will create two new datasets. \n",
    "<br>\n",
    "For now, these are simply copies of <i>integrated_datasets</i>, but they will undergo different modifications later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_multilabel_classification = deepcopy(integrated_datasets)\n",
    "data_for_sentiment_analysis = deepcopy(integrated_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets will undergo the same cleaning process, with some exceptions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_multilabel_classification_clean = clean_dataframe(data_for_multilabel_classification, 'Review')\n",
    "data_for_sentiment_analysis_clean = clean_dataframe(data_for_sentiment_analysis, 'Review', False, False, False, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that *data_for_multilabel_classification_clean* is cleaned, with emojis and punctuation removed, all text converted to lowercase, stop words eliminated, and tokenized. This preprocessing is not applied to *data_for_sentiment_analysis_clean*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first phase of this project focuses on multilabel classification, the following two visualizations were created using the corresponding dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Review Lengths (in Tokens)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1729785944704,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "FpuLIpWYMflf",
    "outputId": "958f2461-468d-4f20-ca5b-6e6b5fe4d358"
   },
   "outputs": [],
   "source": [
    "token_lengths = [len(tokens) for tokens in data_for_multilabel_classification_clean['Review_cleaned']]\n",
    "token_lengths_dataframe = pd.DataFrame(token_lengths, columns = ['Token Length'])\n",
    "plot_histogram(token_lengths_dataframe, x = 'Token Length', nbins = 20, title = 'Visualization of the Size of the Review', labels = {'Token Length': 'Number of Tokens'}, xaxis_title = 'Number of Tokens', yaxis_title = 'Count of Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the majority of reviews (8702) contain between 0 and 49 tokens, while 920 reviews have between 50 and 99 tokens. \n",
    "<br>\n",
    "These are relatively typical review lengths, and we consider them normal for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Most Frequent Words in Cleaned Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 1209,
     "status": "ok",
     "timestamp": 1729785947056,
     "user": {
      "displayName": "Carolina Almeida",
      "userId": "05554083387671212082"
     },
     "user_tz": -60
    },
    "id": "G4Yh_A9XMflj",
    "outputId": "c9f0cdc3-aa96-4b2e-9d29-d674332145e9"
   },
   "outputs": [],
   "source": [
    "data_for_multilabel_classification_clean['Review_cleaned_Temporary'] = data_for_multilabel_classification_clean['Review_cleaned'].apply(lambda x: ' '.join(x)) # Temporary column, just for this visualization\n",
    "\n",
    "donut_chart(top_words(data_for_multilabel_classification_clean, 'Review_cleaned_Temporary', top_n = 10), title = 'Top 10 Words in the column Review (After Cleaning)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_multilabel_classification_clean.drop(columns = ['Review_cleaned_Temporary'], inplace = True) # The temporary column is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial cleaning, we can observe that the most frequent words are <u>good</u>, <u>food</u> and <u>place</u>. \n",
    "<br>\n",
    "These are no longer considered stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>POS Tag and Lemmatization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we convert the tokens in the 'Review_cleaned' column of *data_for_multilabel_classification_clean* into lemmas, as these will be used as input for the model. \n",
    "<br>\n",
    "Additionally, we process the Part-of-Speech (POS) tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_multilabel_classification_clean[['Lemmas', 'POS_Tags']] = data_for_multilabel_classification_clean['Review_cleaned'].apply(lemmatize_tokens).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_treemap(data_for_multilabel_classification_clean['Lemmas'], data_for_multilabel_classification_clean['POS_Tags'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the frequency of different words, this time with the correct POS tags associated with each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code converts all the cuisine entries in the 'Cuisines' column to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_multilabel_classification_clean['Cuisines'] = data_for_multilabel_classification_clean['Cuisines'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Dimensionality Reduction</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we chose to remove words that appear frequently across different types of cuisines, as they do not help in distinguishing between them. \n",
    "<br>\n",
    "Additionally, we eliminated words that were rarely occurring in the dataset, as they could negatively impact the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_multilabel_classification_clean['Lemmas'] = data_for_multilabel_classification_clean['Lemmas'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One_Hot_Encoder =  CountVectorizer(min_df = 40, max_df = 134600, dtype = np.int8, max_features = None, binary = True)\n",
    "\n",
    "words_per_cuisine_type = One_Hot_Encoder.fit_transform(data_for_multilabel_classification_clean['Lemmas'].to_list())\n",
    "\n",
    "words_per_cuisine_type = pd.DataFrame(words_per_cuisine_type.toarray(), columns = One_Hot_Encoder.get_feature_names_out())\n",
    "\n",
    "words_per_cuisine_type.index = data_for_multilabel_classification_clean.index\n",
    "\n",
    "words_per_cuisine_type['Cuisine Type'] = data_for_multilabel_classification_clean['Cuisines'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_cuisine_type['Cuisine Type'] = words_per_cuisine_type['Cuisine Type'].str.split(',')\n",
    "\n",
    "words_per_cuisine_type_exploded = words_per_cuisine_type.explode('Cuisine Type')\n",
    "\n",
    "words_per_cuisine_type_exploded['Cuisine Type'] = words_per_cuisine_type_exploded['Cuisine Type'].str.strip()\n",
    "\n",
    "words_per_cuisine_type_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame above represents the words that appear for each type of cuisine, where 0 indicates the word does not appear and 1 indicates the word is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Clouds for Each Cuisine Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cuisines = words_per_cuisine_type_exploded['Cuisine Type'].unique()\n",
    "\n",
    "plt.figure(figsize = (15, 50))\n",
    "\n",
    "for i, cuisine in enumerate(unique_cuisines):\n",
    "    word_frequencies = generate_word_frequencies_for_cuisine(words_per_cuisine_type_exploded, cuisine)\n",
    "    \n",
    "    plt.subplot(14, 3, i + 1)\n",
    "    wordcloud = word_cloud(word_frequencies)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{cuisine}\", fontsize = 12) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the grid above, we can get an idea of the words that appear for each type of cuisine. \n",
    "<br>\n",
    "At first glance, words like *food*, *good*, *place*, *order*, *ambience*, and *service* seem to appear in most, if not all, of the word clouds. \n",
    "<br>\n",
    "We will remove these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_drop = ['food', 'good', 'place', 'order', 'ambience', 'service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_cuisine_type_exploded_2 = (words_per_cuisine_type_exploded.groupby('Cuisine Type').sum()).T\n",
    "display(words_per_cuisine_type_exploded_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_totals = words_per_cuisine_type_exploded_2.sum()\n",
    "percentages = words_per_cuisine_type_exploded_2.div(cuisine_totals, axis = 1) * 100 \n",
    "display(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame above, we can observe the percentage of each word for each cuisine type. \n",
    "<br>\n",
    "Our goal is to drop words with percentages smaller than 1% or greater than 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_drop = set(words_to_drop) | set(percentages[(percentages.sum(axis = 1) < 1) | (percentages.sum(axis = 1) > 99)].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_multilabel_classification_clean['Lemmas_Treated'] = data_for_multilabel_classification_clean['Lemmas'].apply(lambda lemmas: ' '.join(word for word in lemmas.split() if word not in words_to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Clouds for Each Cuisine Type (After Dimensinality Reduction)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_cuisine_type_exploded.drop(columns = words_to_drop, inplace = True, errors = 'ignore')\n",
    "\n",
    "plt.figure(figsize = (15, 50))\n",
    "\n",
    "for i, cuisine in enumerate(unique_cuisines):\n",
    "    word_frequencies = generate_word_frequencies_for_cuisine(words_per_cuisine_type_exploded, cuisine)\n",
    "    \n",
    "    wordcloud = word_cloud(word_frequencies)\n",
    "    \n",
    "    plt.subplot(14, 3, i + 1)\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{cuisine}\", fontsize = 12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can visualize the most frequent words in the reviews for each cuisine type, this time after applying the previously described process to reduce dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of Word Counts: Lemmas vs Lemmas (Treated)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_count_comparison(data_for_multilabel_classification_clean, 'Lemmas', 'Lemmas_Treated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making these modifications, we observe that exactly <b>36 673</b> lemmas were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>3. Multilabel Classification</font> <a class=\"anchor\" id=\"P3\"></a>\n",
    "  \n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How well can we classify a restaurant’s cuisine type using the content of their reviews as input?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>3.1. Specific Data Preparation</font> <a class=\"anchor\" id=\"P31\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define the <b>data</b> by selecting only the necessary columns: \n",
    "<br>\n",
    "<i>Cuisines</i> (containing the target) and <i>Lemmas_treated</i> (the reviews after all the mentioned modifications). \n",
    "\n",
    "This data is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_for_multilabel_classification_clean[['Cuisines','Lemmas_Treated']]\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Vectorization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will test different types of vectorization, and all of them will be added to our data for testing in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Term-Frequency-Inverse Document Frequency (TF-IDF)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied the TF-IDF (Term Frequency-Inverse Document Frequency) technique to extract the most important features from the text data. \n",
    "<br>\n",
    "Using the `TfidfVectorizer` with a limit of 5000 features and removal of English stop words, we transformed the cleaned text in the Lemmas_Treated column into a TF-IDF matrix, which was then stored in a new column, TF_IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_model = TfidfVectorizer(max_features = 5000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_matrix = TFIDF_model.fit_transform(data['Lemmas_Treated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TF_IDF'] = list(TFIDF_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Word2Vec Skip-gram**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a `Word2Vec` model using the Lemmas_Treated column, where each sentence was split into words. \n",
    "<br>\n",
    "The model was configured with a vector size of 25, a context window of 5 words, and a minimum word count of 1. \n",
    "<br>\n",
    "We used the Skip-gram approach (sg = 1) to learn word embeddings, with 4 worker threads for parallel processing. \n",
    "<br>\n",
    "The resulting word embeddings capture semantic relationships between words in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model = Word2Vec(sentences = data[\"Lemmas_Treated\"].apply(lambda x: x.split()), vector_size = 25, window = 5, min_count = 1, workers = 4, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = ['Category', '\\033[1mtaste\\033[0m']  # Random word\n",
    "\n",
    "table.hrules = HRuleStyle.ALL  \n",
    "table.vrules = VRuleStyle.ALL \n",
    "\n",
    "similarity_result = similarity(Word2Vec_model, 'taste')\n",
    "prediction_result = prediction(Word2Vec_model, 'taste')\n",
    "text_gen_result = text_generator(Word2Vec_model, ['great', 'taste'], 5, random_nr = True, random_nr_max = 10)\n",
    "\n",
    "table.add_row(['Similarity (Most)', f\"{similarity_result[0]} ({similarity_result[1]:.4f})\"])\n",
    "table.add_row(['Similarity (Least)', f\"{similarity_result[2]} ({similarity_result[3]:.4f})\"])\n",
    "table.add_row(['Prediction (Most)', f\"{prediction_result[0]} ({prediction_result[1]:.4f})\"])\n",
    "table.add_row(['Prediction (Least)', f\"{prediction_result[2]} ({prediction_result[3]:.4f})\"])\n",
    "table.add_row(['Text Generator', ', '.join(text_gen_result)]) \n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we applied the Word2Vec model to a randomly selected word, **taste**, to explore various relationships and predictions within the text data. \n",
    "\n",
    "\n",
    "The results were presented in the following categories:\n",
    "\n",
    "- **Similarity (Most):** The word \"preparation\" showed the highest similarity to \"taste\" with a score of 0.9220, indicating a strong semantic connection between the two words.\n",
    "- **Similarity (Least):** The word \"su\" had the lowest similarity to \"taste\" with a score altough 0.4896, suggesting a much weaker relationship.\n",
    "- **Prediction (Most):** The model predicted \"burger\" as the most likely related word, albeit with a very low score of 0.0009, indicating that the prediction strength is not very high.\n",
    "- **Prediction (Least):** The model predicted \"wrap\" with an even lower score of 0.0005, suggesting an even weaker association.\n",
    "- **Text Generator**: Using the words \"great\" and \"taste,\" the model generated a list of related words, including \"coffee,\" \"chocolate,\" \"burger,\" and \"brownie,\" which align with common associations of taste in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_Scatter_Plot_1(Word2Vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_Word2Vec_model = pd.DataFrame(Word2Vec_model.wv.vectors, index = list(Word2Vec_model.wv.key_to_index.keys()))\n",
    "display(vectors_Word2Vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Word2Vec_Skip_gram'] = data['Lemmas_Treated'].apply(lambda x: sentence_vectorizer(x, Word2Vec_model.wv, vector_size = 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Word2Vec Continuous Bag-of-Words (CBOW)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a `Word2Vec` model using the Lemmas_Treated column, where each sentence was split into words.\n",
    "<br>\n",
    "The model was configured with a vector size of 25, a minimum word count of 1, and 4 worker threads for parallel processing.\n",
    "<br>\n",
    "We used the Continuous Bag of Words (CBOW) approach (sg = 0) to learn word embeddings, with 5 epochs for training and a negative sampling exponent (ns_exponent = -1).\n",
    "<br>\n",
    "The resulting word embeddings capture the semantic relationships between words in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_MODEL = Word2Vec(sentences = data[\"Lemmas_Treated\"].apply(lambda x: x.split()), vector_size = 25, min_count = 1, workers = 4, sg = 0, epochs = 5, ns_exponent = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = ['Category', '\\033[1mtaste\\033[0m']  # Random word\n",
    "\n",
    "table.hrules = HRuleStyle.ALL  \n",
    "table.vrules = VRuleStyle.ALL \n",
    "\n",
    "similarity_result = similarity(CBOW_MODEL, 'taste')\n",
    "prediction_result = prediction(CBOW_MODEL, 'taste')\n",
    "text_gen_result = text_generator(CBOW_MODEL, ['great', 'taste'], 5, random_nr = True, random_nr_max = 10)\n",
    "\n",
    "table.add_row(['Similarity (Most)', f\"{similarity_result[0]} ({similarity_result[1]:.4f})\"])\n",
    "table.add_row(['Similarity (Least)', f\"{similarity_result[2]} ({similarity_result[3]:.4f})\"])\n",
    "table.add_row(['Prediction (Most)', f\"{prediction_result[0]} ({prediction_result[1]:.4f})\"])\n",
    "table.add_row(['Prediction (Least)', f\"{prediction_result[2]} ({prediction_result[3]:.4f})\"])\n",
    "table.add_row(['Text Generator', ', '.join(text_gen_result)]) \n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we applied the Word2Vec model to a randomly selected word, **taste**, to analyze its relationships and predictions within the text data.\n",
    "\n",
    "The results were presented in the following categories:\n",
    "\n",
    "- **Similarity (Most):** The word \"well\" showed the highest similarity to \"taste\" with a score of 1.0000, indicating a very strong semantic connection between the two words.\n",
    "- **Similarity (Least):** The word \"jaago\" had the lowest similarity to \"taste\" with a score of 0.6381, suggesting a much weaker relationship.\n",
    "- **Prediction (Most):** The model predicted \"not\" as the most likely related word, with a score of 0.3328, indicating a relatively weak but notable association.\n",
    "- **Prediction (Least):** The model predicted \"restaurant\" with an even lower score of 0.0184, suggesting an even weaker connection.\n",
    "- **Text Generator:** Using the words \"great\" and \"taste,\" the model generated a list of related words, including \"restaurant,\" \"no,\" \"like,\" and \"visit,\" reflecting common associations of taste in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_Scatter_Plot_1(CBOW_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_CBOW_model = pd.DataFrame(CBOW_MODEL.wv.vectors, index = list(CBOW_MODEL.wv.key_to_index.keys()))\n",
    "display(vectors_CBOW_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CBOW'] = data['Lemmas_Treated'].apply(lambda x: sentence_vectorizer(x, CBOW_MODEL.wv, vector_size = 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **GloVe - Pretrained Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiment, we decided to use `GloVe` (Global Vectors for Word Representation), a pre-trained word embedding model, to explore semantic relationships in the text data. \n",
    "<br>\n",
    "Instead of using the standard GloVe embeddings directly from a library, we chose to manually load a retrained GloVe model to have more control over the loading process.\n",
    "\n",
    "We used the manual_loading function to load the GloVe vectors from the file *glove.6B.50d.txt*, which contains 50-dimensional word embeddings for a large vocabulary. \n",
    "<br>\n",
    "This allowed us to work with a specific version of the model that suited our needs, ensuring that we could fine-tune and manipulate the embeddings as required for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = manual_loading('glove/glove.6B.50d.txt') \n",
    "# glove_vectors = manual_loading('glove.6B.50d.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GloVe'] = data['Lemmas_Treated'].apply(lambda lemmas: glove_vectorization(lemmas, glove_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>3.2. Model Implementation</font> <a class=\"anchor\" id=\"P32\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next section, we will explore various combinations of vectorization techniques and machine learning models to identify the best-performing model for multilabel classification.\n",
    "<br>\n",
    "We aim to test different feature representations, such as TF-IDF, Word2Vec, CBOW, and GloVe, alongside various classification algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocessed the target column by splitting the comma-separated values into individual cuisine labels and then applied the `MultiLabelBinarizer` to encode these labels into a binary format, storing the resulting encoded lists in the 'Cuisines_Encoded' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cuisines'] = data['Cuisines'].apply(lambda x: x.split(', '))\n",
    "\n",
    "multi_label_binarizer = MultiLabelBinarizer()\n",
    "\n",
    "encoding = multi_label_binarizer.fit_transform(data['Cuisines'])\n",
    "\n",
    "data['Cuisines_Encoded'] = encoding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Cuisine Types Across the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(words_per_cuisine_type_exploded, 'Cuisine Type', 'Types of Cuisines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the <b>dataset is imbalanced</b>, which will need to be taken into consideration when selecting the metrics for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a comprehensive series of tests is conducted to determine the optimal model, highlighting its superior performance among the alternatives considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Term-Frequency-Inverse Document Frequency (TF-IDF)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our TF-IDF vectorized input into training and validation sets. \n",
    "<br>\n",
    "This process will be repeated for all vectorization methods, resulting in different arrays that will be tested with various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TF_IDF, X_val_TF_IDF, y_train_TF_IDF, y_val_TF_IDF = train_test_split(np.array(data['TF_IDF'].tolist()), encoding, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the splitting process could have introduced some NaN values, we implemented the following code to check for their presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X_train_TF_IDF:\", np.isnan(X_train_TF_IDF).sum())\n",
    "print(\"Missing values in X_val_TF_IDF:\", np.isnan(X_val_TF_IDF).sum())\n",
    "print(\"Missing values in y_train_TF_IDF:\", np.isnan(y_train_TF_IDF).sum() if isinstance(y_train_TF_IDF, np.ndarray) else 0)\n",
    "print(\"Missing values in y_val_TF_IDF:\", np.isnan(y_val_TF_IDF).sum() if isinstance(y_val_TF_IDF, np.ndarray) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, no missing values were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we train several classifiers — *Logistic Regression*, *Decision Tree*, *Multilayer Perceptron*, *Random Forest*, and *Dummy Classifier* — using both the `OneVsRest` and `ClassifierChain` strategies. \n",
    "<br>\n",
    "These strategies are employed to adapt to our multilabel classification problem, where each input can belong to multiple classes. \n",
    "\n",
    "This process will be repeated for all vectorization methods, allowing us to evaluate the performance of different models with various feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "One_vs_Rest_Logistic_Regression_1 = OneVsRestClassifier(LogisticRegression(C = 0.1, solver = 'lbfgs', class_weight = 'balanced')).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "Classifer_Chain_Logistic_Regression_1 = ClassifierChain(LogisticRegression(C = 0.1, solver = 'lbfgs', class_weight = 'balanced')).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "One_vs_Rest_Decision_Tree_Classifier_1 = OneVsRestClassifier(DecisionTreeClassifier(criterion = 'gini', max_depth = 10, min_samples_leaf = 1, min_samples_split = 50)).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "Classifer_Chain_Decision_Tree_Classifier_1 = ClassifierChain(DecisionTreeClassifier(criterion = 'gini', max_depth = 10, min_samples_leaf = 1, min_samples_split = 50)).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "\n",
    "# Multilayer Perceptron Classifier\n",
    "One_vs_Rest_Multilayer_Perceptron_Classifier_1 = OneVsRestClassifier(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "Classifer_Chain_Multilayer_Perceptron_Classifier_1 = ClassifierChain(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "\n",
    "# Random Forest Classifier\n",
    "One_vs_Rest_Random_Forest_Classifier_1 = OneVsRestClassifier(RandomForestClassifier(n_estimators = 50, max_depth = 10, min_samples_leaf = 1, class_weight = 'balanced')).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "Classifer_Chain_Random_Forest_Classifier_1 = ClassifierChain(RandomForestClassifier(n_estimators = 50, max_depth = 10, min_samples_leaf = 1, class_weight = 'balanced')).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "\n",
    "# Dummy Classifier\n",
    "One_vs_Rest_Dummy_Classifier_1 = OneVsRestClassifier(DummyClassifier(strategy = 'most_frequent')).fit(X_train_TF_IDF, y_train_TF_IDF)\n",
    "Classifier_Chain_Dummy_Classifier_1 = ClassifierChain(DummyClassifier(strategy = 'most_frequent')).fit(X_train_TF_IDF, y_train_TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, metrics such as F1 score, precision, and recall were calculated for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for OneVsRest classifiers\n",
    "One_vs_Rest_TF_IDF = {\n",
    "    \"LogisticRegression\": calculate_metrics(One_vs_Rest_Logistic_Regression_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"DecisionTree\": calculate_metrics(One_vs_Rest_Decision_Tree_Classifier_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"MLPClassifier\": calculate_metrics(One_vs_Rest_Multilayer_Perceptron_Classifier_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"RandomForest\": calculate_metrics(One_vs_Rest_Random_Forest_Classifier_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"Dummy\": calculate_metrics(One_vs_Rest_Dummy_Classifier_1, X_val_TF_IDF, y_val_TF_IDF)}\n",
    "\n",
    "# Calculate metrics for ClassifierChain classifiers\n",
    "Classifer_Chain_TF_IDF = {\n",
    "    \"LogisticRegression\": calculate_metrics(Classifer_Chain_Logistic_Regression_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"DecisionTree\": calculate_metrics(Classifer_Chain_Decision_Tree_Classifier_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"MLPClassifier\": calculate_metrics(Classifer_Chain_Multilayer_Perceptron_Classifier_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"RandomForest\": calculate_metrics(Classifer_Chain_Random_Forest_Classifier_1, X_val_TF_IDF, y_val_TF_IDF),\n",
    "    \"Dummy\": calculate_metrics(Classifier_Chain_Dummy_Classifier_1, X_val_TF_IDF, y_val_TF_IDF)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance Comparison for TF-IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance(One_vs_Rest_TF_IDF, Classifer_Chain_TF_IDF, 'Term-Frequency-Inverse Document Frequency (TF-IDF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models outperformed the dummy classifier, which is a positive indication. \n",
    "<br>\n",
    "For this vectorization method, the model with the highest F1 score, precision, and recall appears to be the *MLPClassifier* with a *ClassifierChain* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Word2Vec Skip-gram**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our Word2Vec Skip-fram vectorized input into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Word2Vec_Skip_gram, X_val_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram = train_test_split(np.array(data['Word2Vec_Skip_gram'].tolist()), encoding ,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the splitting process could have introduced some NaN values, we implemented the following code to check for their presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X_train_Word2Vec_Skip_gram:\", np.isnan(X_train_Word2Vec_Skip_gram).sum())\n",
    "print(\"Missing values in X_val_Word2Vec_Skip_gram:\", np.isnan(X_val_Word2Vec_Skip_gram).sum())\n",
    "print(\"Missing values in y_train_Word2Vec_Skip_gram:\", np.isnan(y_train_Word2Vec_Skip_gram).sum() if isinstance(y_train_Word2Vec_Skip_gram, np.ndarray) else 0)\n",
    "print(\"Missing values in y_val_Word2Vec_Skip_gram:\", np.isnan(y_val_Word2Vec_Skip_gram).sum() if isinstance(y_val_Word2Vec_Skip_gram, np.ndarray) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values were detected in the X_train and X_val arrays.\n",
    "<br>\n",
    "These missing values were treated, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train dataset\n",
    "X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram = clean_nan_values(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "\n",
    "# Clean test dataset\n",
    "X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram = clean_nan_values(X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we train several classifiers — *Logistic Regression*, *Decision Tree*, *Multilayer Perceptron*, *Random Forest*, and *Dummy Classifier* — using both the `OneVsRest` and `ClassifierChain` strategies. \n",
    "<br>\n",
    "These strategies are employed to adapt to our multilabel classification problem, where each input can belong to multiple classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "One_vs_Rest_Logistic_Regression_2 = OneVsRestClassifier(LogisticRegression(solver = 'lbfgs', random_state = 0, class_weight = calculate_class_weights(y_train_Word2Vec_Skip_gram))).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "Classifer_Chain_Logistic_Regression_2 = ClassifierChain(LogisticRegression(solver = 'lbfgs', random_state = 0, class_weight = calculate_class_weights(y_train_Word2Vec_Skip_gram))).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "One_vs_Rest_Decision_Tree_Classifier_2 = OneVsRestClassifier(DecisionTreeClassifier(random_state = 42, criterion = 'gini', max_depth = 20, min_samples_leaf = 10, min_samples_split = 150, class_weight = calculate_class_weights(y_train_Word2Vec_Skip_gram))).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "Classifer_Chain_Decision_Tree_Classifier_2 = ClassifierChain(DecisionTreeClassifier(random_state = 42, criterion = 'gini', max_depth = 20, min_samples_leaf = 10, min_samples_split = 150, class_weight = calculate_class_weights(y_train_Word2Vec_Skip_gram))).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "\n",
    "# Multilayer Perceptron Classifier\n",
    "One_vs_Rest_Multilayer_Perceptron_Classifier_2 = OneVsRestClassifier(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "Classifer_Chain_Multilayer_Perceptron_Classifier_2 = ClassifierChain(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "\n",
    "# Random Forest Classifier\n",
    "One_vs_Rest_Random_Forest_Classifier_2 = OneVsRestClassifier(RandomForestClassifier(random_state = 42, criterion = 'log_loss', n_estimators = 100, max_depth = 10, min_samples_leaf = 5, class_weight = calculate_class_weights(y_train_Word2Vec_Skip_gram))).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "Classifer_Chain_Random_Forest_Classifier_2 = ClassifierChain(RandomForestClassifier(random_state = 42, criterion = 'log_loss', n_estimators = 100, max_depth = 10, min_samples_leaf = 5, class_weight = calculate_class_weights(y_train_Word2Vec_Skip_gram))).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "\n",
    "# Dummy Classifier\n",
    "One_vs_Rest_Dummy_Classifier_2 = OneVsRestClassifier(DummyClassifier(strategy = 'most_frequent')).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)\n",
    "Classifier_Chain_Dummy_Classifier_2 = ClassifierChain(DummyClassifier(strategy = 'most_frequent')).fit(X_train_Word2Vec_Skip_gram, y_train_Word2Vec_Skip_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, metrics such as F1 score, precision, and recall were calculated for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for OneVsRest classifiers\n",
    "One_vs_Rest_Word2Vec_Skip_gram = {\n",
    "    \"LogisticRegression\": calculate_metrics(One_vs_Rest_Logistic_Regression_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"DecisionTree\": calculate_metrics(One_vs_Rest_Decision_Tree_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"MLPClassifier\": calculate_metrics(One_vs_Rest_Multilayer_Perceptron_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"RandomForest\": calculate_metrics(One_vs_Rest_Random_Forest_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"Dummy\": calculate_metrics(One_vs_Rest_Dummy_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram)}\n",
    "\n",
    "# Calculate metrics for ClassifierChain classifiers\n",
    "Classifer_Chain_Word2Vec_Skip_gram = {\n",
    "    \"LogisticRegression\": calculate_metrics(Classifer_Chain_Logistic_Regression_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"DecisionTree\": calculate_metrics(Classifer_Chain_Decision_Tree_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"MLPClassifier\": calculate_metrics(Classifer_Chain_Multilayer_Perceptron_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"RandomForest\": calculate_metrics(Classifer_Chain_Random_Forest_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram),\n",
    "    \"Dummy\": calculate_metrics(Classifier_Chain_Dummy_Classifier_2, X_val_Word2Vec_Skip_gram, y_val_Word2Vec_Skip_gram)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance Comparison for Word2Vec Skip-gram Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance(One_vs_Rest_Word2Vec_Skip_gram, Classifer_Chain_Word2Vec_Skip_gram, 'Word2Vec Skip-gram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the comparison with the dummy classifier is not as favorable, as the MLPClassifier shows similar performance. \n",
    "<br>\n",
    "Overall, the scores for the other models appear lower compared to the TF-IDF method. \n",
    "<br>\n",
    "However, the best-performing model seems to be the *RandomForest* with a *OneVsRest* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Word2Vec Continuous Bag-of-Words (CBOW)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our CBOW vectorized input into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CBOW, X_val_CBOW, y_train_CBOW, y_val_CBOW = train_test_split(np.array(data['CBOW'].tolist()), encoding ,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the splitting process could have introduced some NaN values, we implemented the following code to check for their presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X_train_CBOW:\", np.isnan(X_train_CBOW).sum())\n",
    "print(\"Missing values in X_val_CBOW:\", np.isnan(X_val_CBOW).sum())\n",
    "print(\"Missing values in y_train_CBOW:\", np.isnan(y_train_CBOW).sum() if isinstance(y_train_CBOW, np.ndarray) else 0)\n",
    "print(\"Missing values in y_val_CBOW:\", np.isnan(y_val_CBOW).sum() if isinstance(y_val_CBOW, np.ndarray) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values were detected in the X_train and X_val arrays.\n",
    "<br>\n",
    "These missing values were treated, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train dataset\n",
    "X_train_CBOW, y_train_CBOW = clean_nan_values(X_train_CBOW, y_train_CBOW)\n",
    "\n",
    "# Clean test dataset\n",
    "X_val_CBOW, y_val_CBOW = clean_nan_values(X_val_CBOW, y_val_CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we train several classifiers — *Logistic Regression*, *Decision Tree*, *Multilayer Perceptron*, *Random Forest*, and *Dummy Classifier* — using both the `OneVsRest` and `ClassifierChain` strategies. \n",
    "<br>\n",
    "These strategies are employed to adapt to our multilabel classification problem, where each input can belong to multiple classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "One_vs_Rest_Logistic_Regression_3 = OneVsRestClassifier(LogisticRegression(solver = 'lbfgs', random_state = 0, class_weight = calculate_class_weights(y_train_CBOW))).fit(X_train_CBOW, y_train_CBOW)\n",
    "Classifer_Chain_Logistic_Regression_3 = ClassifierChain(LogisticRegression(solver = 'lbfgs', random_state = 0, class_weight = calculate_class_weights(y_train_CBOW))).fit(X_train_CBOW, y_train_CBOW)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "One_vs_Rest_Decision_Tree_Classifier_3 = OneVsRestClassifier(DecisionTreeClassifier(random_state = 42, criterion = 'gini', max_depth = 20, min_samples_leaf = 10, min_samples_split = 150, class_weight = calculate_class_weights(y_train_CBOW))).fit(X_train_CBOW, y_train_CBOW)\n",
    "Classifer_Chain_Decision_Tree_Classifier_3 = ClassifierChain(DecisionTreeClassifier(random_state = 42, criterion = 'gini', max_depth = 20, min_samples_leaf = 10, min_samples_split = 150, class_weight = calculate_class_weights(y_train_CBOW))).fit(X_train_CBOW, y_train_CBOW)\n",
    "\n",
    "# Multilayer Perceptron Classifier\n",
    "One_vs_Rest_Multilayer_Perceptron_Classifier_3 = OneVsRestClassifier(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_CBOW, y_train_CBOW)\n",
    "Classifer_Chain_Multilayer_Perceptron_Classifier_3 = ClassifierChain(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_CBOW, y_train_CBOW)\n",
    "\n",
    "# Random Forest Classifier\n",
    "One_vs_Rest_Random_Forest_Classifier_3 = OneVsRestClassifier(RandomForestClassifier(random_state = 42, criterion = 'log_loss', n_estimators = 100, max_depth = 10, min_samples_leaf = 5, class_weight = calculate_class_weights(y_train_CBOW))).fit(X_train_CBOW, y_train_CBOW)\n",
    "Classifer_Chain_Random_Forest_Classifier_3 = ClassifierChain(RandomForestClassifier(random_state = 42, criterion = 'log_loss', n_estimators = 100, max_depth = 10, min_samples_leaf = 5, class_weight = calculate_class_weights(y_train_CBOW))).fit(X_train_CBOW, y_train_CBOW)\n",
    "\n",
    "# Dummy Classifier\n",
    "One_vs_Rest_Dummy_Classifier_3 = OneVsRestClassifier(DummyClassifier(strategy = 'most_frequent')).fit(X_train_CBOW, y_train_CBOW)\n",
    "Classifier_Chain_Dummy_Classifier_3 = ClassifierChain(DummyClassifier(strategy = 'most_frequent')).fit(X_train_CBOW, y_train_CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, metrics such as F1 score, precision, and recall were calculated for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for OneVsRest classifiers\n",
    "One_vs_Rest_CBOW = {\n",
    "    \"LogisticRegression\": calculate_metrics(One_vs_Rest_Logistic_Regression_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"DecisionTree\": calculate_metrics(One_vs_Rest_Decision_Tree_Classifier_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"MLPClassifier\": calculate_metrics(One_vs_Rest_Multilayer_Perceptron_Classifier_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"RandomForest\": calculate_metrics(One_vs_Rest_Random_Forest_Classifier_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"Dummy\": calculate_metrics(One_vs_Rest_Dummy_Classifier_3, X_val_CBOW, y_val_CBOW)}\n",
    "\n",
    "# Calculate metrics for ClassifierChain classifiers\n",
    "Classifer_Chain_CBOW = {\n",
    "    \"LogisticRegression\": calculate_metrics(Classifer_Chain_Logistic_Regression_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"DecisionTree\": calculate_metrics(Classifer_Chain_Decision_Tree_Classifier_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"MLPClassifier\": calculate_metrics(Classifer_Chain_Multilayer_Perceptron_Classifier_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"RandomForest\": calculate_metrics(Classifer_Chain_Random_Forest_Classifier_3, X_val_CBOW, y_val_CBOW),\n",
    "    \"Dummy\": calculate_metrics(Classifier_Chain_Dummy_Classifier_3, X_val_CBOW, y_val_CBOW)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance Comparison for Word2Vec Continuous Bag-of-Words (CBOW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance(One_vs_Rest_CBOW, Classifer_Chain_CBOW, 'Word2Vec Continuous Bag-of-Words (CBOW)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, once again, the MLPClassifier shows much lower performance, and the other models also perform worse than in previous cases. \n",
    "<br>\n",
    "The best model, which is the *RandomForest* with a *OneVsRest* classifier, exhibits a significant disparity between recall and both precision and F1 score, indicating that it won't be a suitable choice for the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **GloVe - Pretrained Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our GloVe vectorized input into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_GloVe, X_val_GloVe, y_train_GloVe, y_val_GloVe = train_test_split(np.array(data['GloVe'].tolist()), encoding, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the splitting process could have introduced some NaN values, we implemented the following code to check for their presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X_train_GloVe:\", np.isnan(X_train_GloVe).sum())\n",
    "print(\"Missing values in X_val_GloVe:\", np.isnan(X_val_GloVe).sum())\n",
    "print(\"Missing values in y_train_GloVe:\", np.isnan(y_train_GloVe).sum() if isinstance(y_train_GloVe, np.ndarray) else 0)\n",
    "print(\"Missing values in y_val_GloVe:\", np.isnan(y_val_GloVe).sum() if isinstance(y_val_GloVe, np.ndarray) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, no missing values were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we train several classifiers — *Logistic Regression*, *Decision Tree*, *Multilayer Perceptron*, *Random Forest*, and *Dummy Classifier* — using both the `OneVsRest` and `ClassifierChain` strategies. \n",
    "<br>\n",
    "These strategies are employed to adapt to our multilabel classification problem, where each input can belong to multiple classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "One_vs_Rest_Logistic_Regression_4 = OneVsRestClassifier(LogisticRegression(solver = 'lbfgs', random_state = 0, class_weight = calculate_class_weights(y_train_GloVe))).fit(X_train_GloVe, y_train_GloVe)\n",
    "Classifer_Chain_Logistic_Regression_4 = ClassifierChain(LogisticRegression(solver = 'lbfgs', random_state = 0, class_weight = calculate_class_weights(y_train_GloVe))).fit(X_train_GloVe, y_train_GloVe)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "One_vs_Rest_Decision_Tree_Classifier_4 = OneVsRestClassifier(DecisionTreeClassifier(random_state = 42, criterion = 'gini', max_depth = 20, min_samples_leaf = 10, min_samples_split = 150, class_weight = calculate_class_weights(y_train_GloVe))).fit(X_train_GloVe, y_train_GloVe)\n",
    "Classifer_Chain_Decision_Tree_Classifier_4 = ClassifierChain(DecisionTreeClassifier(random_state = 42, criterion = 'gini', max_depth = 20, min_samples_leaf = 10, min_samples_split = 150, class_weight = calculate_class_weights(y_train_GloVe))).fit(X_train_GloVe, y_train_GloVe)\n",
    "\n",
    "# Multilayer Perceptron Classifier\n",
    "One_vs_Rest_Multilayer_Perceptron_Classifier_4 = OneVsRestClassifier(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_GloVe, y_train_GloVe)\n",
    "Classifer_Chain_Multilayer_Perceptron_Classifier_4 = ClassifierChain(MLPClassifier(random_state = 42, hidden_layer_sizes = (128, 32), activation = 'relu', solver = 'adam', alpha = 0.001, max_iter = 70, early_stopping = True, learning_rate_init = 0.001)).fit(X_train_GloVe, y_train_GloVe)\n",
    "\n",
    "# Random Forest Classifier\n",
    "One_vs_Rest_Random_Forest_Classifier_4 = OneVsRestClassifier(RandomForestClassifier(random_state = 42, criterion = 'log_loss', n_estimators = 100, max_depth = 10, min_samples_leaf = 5, class_weight = calculate_class_weights(y_train_GloVe))).fit(X_train_GloVe, y_train_GloVe)\n",
    "Classifer_Chain_Random_Forest_Classifier_4 = ClassifierChain(RandomForestClassifier(random_state = 42, criterion = 'log_loss', n_estimators = 100, max_depth = 10, min_samples_leaf = 5, class_weight = calculate_class_weights(y_train_GloVe))).fit(X_train_GloVe, y_train_GloVe)\n",
    "\n",
    "# Dummy Classifier\n",
    "One_vs_Rest_Dummy_Classifier_4 = OneVsRestClassifier(DummyClassifier(strategy = 'most_frequent')).fit(X_train_GloVe, y_train_GloVe)\n",
    "Classifier_Chain_Dummy_Classifier_4 = ClassifierChain(DummyClassifier(strategy = 'most_frequent')).fit(X_train_GloVe, y_train_GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, metrics such as F1 score, precision, and recall were calculated for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for OneVsRest classifiers\n",
    "One_vs_Rest_GloVe = {\n",
    "    \"LogisticRegression\": calculate_metrics(One_vs_Rest_Logistic_Regression_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"DecisionTree\": calculate_metrics(One_vs_Rest_Decision_Tree_Classifier_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"MLPClassifier\": calculate_metrics(One_vs_Rest_Multilayer_Perceptron_Classifier_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"RandomForest\": calculate_metrics(One_vs_Rest_Random_Forest_Classifier_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"Dummy\": calculate_metrics(One_vs_Rest_Dummy_Classifier_4, X_val_GloVe, y_val_GloVe)}\n",
    "\n",
    "# Calculate metrics for ClassifierChain classifiers\n",
    "Classifer_Chain_GloVe = {\n",
    "    \"LogisticRegression\": calculate_metrics(Classifer_Chain_Logistic_Regression_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"DecisionTree\": calculate_metrics(Classifer_Chain_Decision_Tree_Classifier_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"MLPClassifier\": calculate_metrics(Classifer_Chain_Multilayer_Perceptron_Classifier_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"RandomForest\": calculate_metrics(Classifer_Chain_Random_Forest_Classifier_4, X_val_GloVe, y_val_GloVe),\n",
    "    \"Dummy\": calculate_metrics(Classifier_Chain_Dummy_Classifier_4, X_val_GloVe, y_val_GloVe)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance Comparison for GloVe - Pretrained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance(One_vs_Rest_GloVe, Classifer_Chain_GloVe, 'GloVe - Pretrained Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the MLPClassifier shows poor performance, and the other models also do not perform exceptionally well. \n",
    "<br>\n",
    "The best model appears to be the *RandomForest* with a *ClassifierChain* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>3.3. Model Evaluation</font> <a class=\"anchor\" id=\"P33\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis from the plots above, we have decided that our final model should be a <b>Multilayer Perceptron Classifier</b> using the <b>TF-IDF</b> vectorizer. \n",
    "<br>\n",
    "This choice was made as it consistently outperformed the dummy classifier by the widest margin across the evaluated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifer_Chain_Multilayer_Perceptron_Classifier_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Overfitting in the Multilayer Perceptron Classifier (Classifier Chain) with TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_overfitting(Classifer_Chain_Multilayer_Perceptron_Classifier_1, X_train_TF_IDF, y_train_TF_IDF, X_val_TF_IDF, y_val_TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the training data outperforms the validation data across all metrics, with a significant gap. \n",
    "<br>\n",
    "One approach to address this issue is oversampling, which we will apply next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Oversampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we applied **random oversampling** to address the class imbalance in our training and validation datasets. \n",
    "\n",
    "First, we combined the feature and label arrays for both the training and validation sets. \n",
    "<br>\n",
    "Then, using the `resample` function, we oversampled the data to match the original size of the training set, ensuring a balanced representation of classes. \n",
    "<br>\n",
    "The resulting oversampled datasets were then split back into their respective feature (X) and label (y) components. \n",
    "\n",
    "This technique helps to mitigate the risk of overfitting and improve the model's generalization by providing a more balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_X_y_train = np.hstack((X_train_TF_IDF, y_train_TF_IDF))\n",
    "Combined_X_y_val = np.hstack((X_val_TF_IDF, y_val_TF_IDF))\n",
    "\n",
    "oversampled_train_data = resample(Combined_X_y_train, replace = True, n_samples = len(X_train_TF_IDF), random_state = 42)\n",
    "oversampled_val_data = resample(Combined_X_y_val, replace = True, n_samples = len(X_val_TF_IDF), random_state = 42)\n",
    "\n",
    "X_train_TF_IDF_random_oversample = oversampled_train_data[:, : - y_train_TF_IDF.shape[1]]\n",
    "y_train_TF_IDF_random_oversample = oversampled_train_data[:, - y_train_TF_IDF.shape[1]:]\n",
    "\n",
    "X_val_TF_IDF_random_oversample = oversampled_val_data[:, : - y_val_TF_IDF.shape[1]]\n",
    "y_val_TF_IDF_random_oversample = oversampled_val_data[:, - y_val_TF_IDF.shape[1]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating Model Performance After Random Oversampling for Multilayer Perceptron Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_overfitting(Classifer_Chain_Multilayer_Perceptron_Classifier_1, X_train_TF_IDF_random_oversample, y_train_TF_IDF_random_oversample, X_val_TF_IDF_random_oversample, y_val_TF_IDF_random_oversample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after applying random oversampling to address the class imbalance in the training and validation datasets, we did not observe a significant improvement in performance. \n",
    "<br>\n",
    "Despite balancing the classes, the model's results remained largely unchanged, suggesting that oversampling may not have been the most effective solution in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluating Performance for Each Cuisine Type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the main objective of this model is to assess how well it can predict each type of cuisine. \n",
    "<br>\n",
    "To achieve this, we will present separate metrics for each label, as this approach is more suitable given the varying levels of class imbalance across the different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the Training Set\n",
    "train_predictions = Classifer_Chain_Multilayer_Perceptron_Classifier_1.predict(X_train_TF_IDF)\n",
    "\n",
    "Classification_Report_Train = classification_report(y_train_TF_IDF, train_predictions, target_names = multi_label_binarizer.classes_, output_dict = True)\n",
    "\n",
    "Cuisine_Type_Performances_Train = pd.DataFrame(Classification_Report_Train).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the Validation Set\n",
    "validation_predictions = Classifer_Chain_Multilayer_Perceptron_Classifier_1.predict(X_val_TF_IDF)\n",
    "\n",
    "Classification_Report_Validation = classification_report(y_val_TF_IDF, validation_predictions, target_names = multi_label_binarizer.classes_, output_dict = True)\n",
    "\n",
    "Cuisine_Type_Performances_Validation = pd.DataFrame(Classification_Report_Validation).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following code, we created a DataFrame that displays the F1 scores for both the training and validation sets for each type of cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cuisine_Type_Performances_Train_Filter = Cuisine_Type_Performances_Train.loc[multi_label_binarizer.classes_]\n",
    "Cuisine_Type_Performances_Validation_Filter = Cuisine_Type_Performances_Validation.loc[multi_label_binarizer.classes_]\n",
    "\n",
    "f1_scores = pd.DataFrame({'Cuisine Type': multi_label_binarizer.classes_, 'F1 Score (Train)': Cuisine_Type_Performances_Train_Filter['f1-score'], 'F1 Score (Validation)': Cuisine_Type_Performances_Validation_Filter['f1-score']}).set_index('Cuisine Type')\n",
    "\n",
    "display(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Scores for Each Cuisine Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_Scores_Cuisine_Types(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite our efforts, we observed that the most frequent cuisine types achieved better F1 scores, while the less frequent cuisines, such as Mexican, had a score of 0. \n",
    "<br>\n",
    "This indicates that the model struggles with predicting these less represented classes. \n",
    "<br>\n",
    "Additionally, overfitting remains a significant issue, as the model performs much better on the training data compared to the validation data. \n",
    "\n",
    "Overall, we did not achieve the desired results, and further improvements are needed to address these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>4. Sentiment Analysis</font> <a class=\"anchor\" id=\"P4\"></a>\n",
    "  \n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How well can we predict a restaurant’s Zomato score using the polarity of their reviews as input?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this section is to apply sentiment analysis models to assess the polarity of the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>4.1. Specific Data Preparation</font> <a class=\"anchor\" id=\"P41\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define the <b>data_</b> by selecting only the necessary columns: \n",
    "<br>\n",
    "<i>Cuisines</i>, <i>Review_cleaned</i> and <i>Rating</i>\n",
    "\n",
    "This data is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = data_for_sentiment_analysis_clean[['Cuisines', 'Review_cleaned', 'Rating']]\n",
    "display(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>4.2. Model Implementation</font> <a class=\"anchor\" id=\"P42\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <b>Valence Aware Dictionary and sEntiment Reasoner (VADER)</b>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we used the VADER sentiment analysis tool to analyze the polarity of the cleaned reviews.\n",
    "<br>\n",
    "`The SentimentIntensityAnalyzer` was applied to the Review_cleaned column, with the sentiment scores averaged across sentences for each review. \n",
    "<br>\n",
    "The results were then displayed in the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vader_Sentiment_Analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = Apply_Vader(data_, column = 'Review_cleaned', mean_sentence = True)\n",
    "\n",
    "display(data_.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we to generate summary statistics for the sentiment scores. \n",
    "<br>\n",
    "This helped us get an overview of the distribution of both the individual compound scores and the mean compound scores for each review, allowing us to better understand the overall sentiment of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_[['Compound Score (VADER)', 'Mean Compound Score (VADER)']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Skewed Sentiment Distribution:** The mean of the Compound Score (VADER) is 0.4876, indicating a general positive sentiment across the dataset. Most reviews appear to express a positive sentiment overall. The Mean Compound Score (VADER) has a lower mean of 0.2833, suggesting that the overall sentiment of the reviews may be somewhat less positive on average when considering the sentence-level sentiment.\n",
    "\n",
    "- **High Variability:** Both the Compound Score (VADER) and Mean Compound Score (VADER) show significant variability, with a high standard deviation (0.5796 and 0.3734 respectively). This suggests a wide range of sentiments across the reviews, with many reviews showing highly positive or negative sentiments.\n",
    "\n",
    "- **Sentiment Range:** The minimum value for both scores is negative (around -0.99), indicating some reviews have strong negative sentiments. The maximum values for both scores are near 1 (0.9997 for Compound Score and 0.9956 for Mean Compound Score), reflecting reviews with very positive sentiments.\n",
    "\n",
    "- **Sentiment Concentration:** The 50th percentile (median) of the Compound Score (VADER) is 0.7841, suggesting that half of the reviews are more positive than this value. For the Mean Compound Score (VADER), the median is 0.3280, reflecting that the sentence-level sentiment tends to be less positive but still generally favorable.\n",
    "\n",
    "- **Positive Sentiment Dominance:** The higher 75th percentile for the Compound Score (VADER) (0.9393) further supports that a majority of reviews lean towards the positive side, while a significant portion of the dataset also exhibits relatively strong positive sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Pearson correlation` between the *Compound Score (VADER)* and *Mean Compound Score (VADER)* was computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_between_scores = compute_pearson_correlation(data_, 'Compound Score (VADER)', 'Mean Compound Score (VADER)')\n",
    "print(f\"Pearson correlation between 'Compound Score' and 'Mean Compound Score': {correlation_between_scores:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It resulted in a value of 0.82, indicating a strong positive linear relationship between the overall and sentence-level sentiment scores.\n",
    "\n",
    "This relationship is illustrated in the plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Comparison Between Compound Score and Mean Compound Score (VADER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_comparison(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Sentiment Scores Using VADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = deepcopy(data_)\n",
    "\n",
    "data_to_plot = extract_scores(data_to_plot, review_column = 'Review_cleaned')\n",
    "\n",
    "plot_histogram_VADER(data_to_plot, positive_column = 'Positive', negative_column = 'Negative', neutral_column = 'Neutral', compound_column = 'Compound Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis Heatmap Using VADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap_VADER(data_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows a strong negative correlation between the neutral and positive sentiment scores. \n",
    "<br>\n",
    "This suggests that as a review's sentiment becomes more positive, it tends to have a lower neutral score, and vice versa. \n",
    "<br>\n",
    "This pattern may indicate that reviews with more pronounced positive sentiments tend to express more extreme opinions, while neutral reviews lack significant sentiment, further strengthening this inverse relationship. \n",
    "\n",
    "Understanding such correlations can be helpful in refining sentiment analysis models to better interpret review data and improve accuracy in sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Scores by Cuisine Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_['Cuisines_List'] = data_['Cuisines'].apply(lambda x: x.split(', '))\n",
    "\n",
    "exploded_data_ = data_.explode('Cuisines_List').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_per_cuisine(exploded_data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram for compound scores across different types of cuisines shows a skew towards positive reviews for all cuisines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <b>TextBlob</b>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we used the TextBlob sentiment analysis tool to analyze the polarity of the cleaned reviews.\n",
    "<br>\n",
    "A custom function was applied to the Review_cleaned column to calculate the sentiment scores for each review.\n",
    "<br>\n",
    "The results were then displayed in the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = Apply_TextBlob(data_, column = 'Review_cleaned')\n",
    "\n",
    "display(data_.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Polarity and Subjectivity from TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_TextBlob(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment categorizes the reviews based on their sentiment and subjectivity scores obtained from TextBlob analysis. Reviews are divided into different groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_positive_reviews = data_[data_['Polarity Score (TextBlob)'] > 0.7]\n",
    "highly_negative_reviews = data_[data_['Polarity Score (TextBlob)'] < - 0.7]\n",
    "\n",
    "neutral_reviews = data_[(data_['Polarity Score (TextBlob)'] > -0.1) & (data_['Polarity Score (TextBlob)'] < 0.1)]\n",
    "\n",
    "highly_subjective_reviews = data_[data_['Subjectivity Score (TextBlob)'] > 0.9]\n",
    "highly_objective_reviews = data_[data_['Subjectivity Score (TextBlob)']< 0.2]\n",
    "\n",
    "print(\"Highly Positive Reviews:\", len(highly_positive_reviews))\n",
    "print(\"Highly Negative Reviews:\", len(highly_negative_reviews))\n",
    "print(\"Neutral Reviews:\", len(neutral_reviews))\n",
    "print(\"Highly Subjective Reviews:\", len(highly_subjective_reviews))\n",
    "print(\"Highly Objective Reviews:\", len(highly_objective_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the categorization is as follows:\n",
    "\n",
    "- **Highly Positive Reviews:** 659 reviews have a polarity score greater than 0.7, indicating strong positive sentiment.\n",
    "- **Highly Negative Reviews:** 140 reviews have a polarity score less than -0.7, indicating strong negative sentiment.\n",
    "- **Neutral Reviews:** 1658 reviews have a polarity score between -0.1 and 0.1, reflecting a neutral sentiment.\n",
    "- **Highly Subjective Reviews:** 482 reviews have a subjectivity score greater than 0.9, suggesting they are highly subjective and personal.\n",
    "- **Highly Objective Reviews:** 559 reviews have a subjectivity score less than 0.2, indicating they are highly objective and factual.\n",
    "\n",
    "These results show the distribution of sentiment and subjectivity in the reviews, with most of the reviews being neutral and subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polarity Scores by Cuisine Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_['Cuisines_List'] = data_['Cuisines'].apply(lambda x: x.split(', '))\n",
    "\n",
    "exploded_data_ = data_.explode('Cuisines_List').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_per_cuisine(exploded_data_, vader = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All histograms of the polarity scores seem to have a relatively normal distribution for each type of cuisine. \n",
    "<br>\n",
    "This suggests that the sentiment expressed in the reviews is evenly distributed across the different cuisines, with reviews generally having a balanced mix of positive, neutral, and negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>4.3. Model Evaluation</font> <a class=\"anchor\" id=\"P43\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the scores from these models with the actual ratings. \n",
    "\n",
    "First, we check for any missing or infinite values in the desired columns, as there could have been a typo during the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing Values:')\n",
    "print(data_[['Compound Score (VADER)', 'Mean Compound Score (VADER)', 'Polarity Score (TextBlob)', 'Subjectivity Score (TextBlob)']].isnull().sum())\n",
    "print() \n",
    "print('Infinite Values:')\n",
    "print(np.isinf(data_[['Compound Score (VADER)', 'Mean Compound Score (VADER)', 'Polarity Score (TextBlob)', 'Subjectivity Score (TextBlob)']]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the 'Mean Compound Score (VADER)' column had 8 missing values, which will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = data_.dropna(subset = ['Mean Compound Score (VADER)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we apply the `MinMaxScaler` to normalize the values of the 'Rating' column and the sentiment scores ('Compound Score (VADER)' and 'Polarity Score (TextBlob)'). \n",
    "<br>\n",
    "This ensures that all values are within the range [0, 1], making them comparable. \n",
    "<br>\n",
    "We also check for any missing values in the scaled columns to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_Max_Scaler = MinMaxScaler()\n",
    "\n",
    "Rating_Scaled = Min_Max_Scaler.fit_transform(data_['Rating'].to_numpy().reshape(-1, 1))\n",
    "Compound_Score_Vader_Scaled = Min_Max_Scaler.fit_transform(data_['Compound Score (VADER)'].to_numpy().reshape(-1, 1))\n",
    "Polarity_Score_TextBlob_Scaled = Min_Max_Scaler.fit_transform(data_['Polarity Score (TextBlob)'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "print(\"Missing Values in Rating_Scaled:\", np.isnan(Rating_Scaled).sum())\n",
    "print(\"Missing Values in Compound_Score_Vader_Scaled:\", np.isnan(Compound_Score_Vader_Scaled).sum())\n",
    "print(\"Missing Values in Polarity_Score_TextBlob_Scaled:\", np.isnan(Polarity_Score_TextBlob_Scaled).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code removes the row with missing values in the '+'Rating'+' column and drops the corresponding rows in the sentiment score columns, ensuring data consistency before further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(Rating_Scaled.reshape(-1)) & ~np.isnan(Compound_Score_Vader_Scaled.reshape(-1)) & ~np.isnan(Polarity_Score_TextBlob_Scaled.reshape(-1))\n",
    "\n",
    "Rating_Scaled, Compound_Score_Vader_Scaled, Polarity_Score_TextBlob_Scaled = [x[mask] for x in [Rating_Scaled, Compound_Score_Vader_Scaled, Polarity_Score_TextBlob_Scaled]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was used to compute sentiment analysis metrics by comparing the scaled values of the 'Rating' column with the scaled sentiment scores from VADER and TextBlob. \n",
    "\n",
    "The resulting metrics were displayed for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_metrics = Sentiment_Analysis_Metrics(Rating_Scaled, Compound_Score_Vader_Scaled, Polarity_Score_TextBlob_Scaled)\n",
    "display(sentiment_analysis_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Error Metrics:** Both models demonstrate relatively low error rates, with VADER showing slightly better results than TextBlob in most cases. For example, VADER has a lower Mean Squared Error (0.0794) and Root Mean Squared Error (0.2818) compared to TextBlob's MSE (0.0773) and RMSE (0.2780). Similarly, VADER has a lower Mean Absolute Error (0.2080) than TextBlob (0.2297).\n",
    "- **Correlation:** The Pearson correlation between sentiment scores and ratings is reasonably strong for both models, with VADER scoring slightly higher (0.7018) than TextBlob (0.6957). This indicates a good positive relationship between sentiment scores and ratings.\n",
    "\n",
    "Overall, while both models align reasonably well with actual ratings, VADER shows a slight edge in performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Plot: VADER vs. TextBlob Sentiment Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression(Compound_Score_Vader_Scaled, Polarity_Score_TextBlob_Scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, a positive correlation is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalized Distributions of Rating and Sentiment Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normalized_distributions(Rating_Scaled, Compound_Score_Vader_Scaled, Polarity_Score_TextBlob_Scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve for the rating and the Compound Score (VADER) appears similar, while it differs significantly from the Polarity Score (TextBlob). \n",
    "<br>\n",
    "This supports our conclusion that VADER demonstrates better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>5. Topic Modelling</font> <a class=\"anchor\" id=\"P5\"></a>\n",
    "  \n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Can the reviews be classified according to emergent topics? (e.g., can review j be made up of 0.5 topic “service; speed; sympathy”, and 0.3 topic “ambiance; decoration; furniture”?) What do the emergent topic mean? (i.e., are they meaningful regarding the project’s context?) Can relevant insights be extracted from the topics?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we perform topic modeling on the preprocessed text data (the same used for the multilabel classifiaction) using various vectorization methods and models.\n",
    "<br>\n",
    " By applying Bag of Words (BoW) and TF-IDF for text representation, and `Latent Semantic Analysis` (LSA) and `Latent Dirichlet Allocation` (LDA) for modeling, we aim to identify underlying topics within the data. \n",
    " <br>\n",
    " The analysis explores up to 5 topics, extracting the top 10 words for each, using the Lemmas_Treated column as the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_Modelling = run_topic_modelling(data = data_for_multilabel_classification_clean, vectorization_methods = ['bow', 'tfidf'], models = ['LSA', 'LDA'], max_k = 5, n_top_words = 10, text_column = 'Lemmas_Treated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BoW-based methods (both LSA and LDA) generally produced higher coherence scores, making them more effective for extracting meaningful topics from this dataset.\n",
    "\n",
    "- TF-IDF-based methods resulted in lower coherence scores, indicating that the additional weighting of terms may not have been as effective in this context.\n",
    "\n",
    "Overall, the results suggest that **LSA with BoW** performed the best in terms of topic coherence and interpretability, offering a clear view of the most prominent themes, such as food quality, service experience, and delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows = 2, cols = 2, subplot_titles = [\"LSA & TFIDF\", \"LSA & BOW\", \"LDA & TFIDF\", \"LDA & BOW\"], shared_yaxes = True, shared_xaxes = False)\n",
    "\n",
    "fig.add_trace(heatmap_topic_modelling(data_for_multilabel_classification_clean, 'LSA', 'TFIDF'), row = 1, col = 1)\n",
    "fig.add_trace(heatmap_topic_modelling(data_for_multilabel_classification_clean, 'LSA', 'BOW'), row = 1, col = 2)\n",
    "fig.add_trace(heatmap_topic_modelling(data_for_multilabel_classification_clean, 'LDA', 'TFIDF'), row = 2, col = 1)\n",
    "fig.add_trace(heatmap_topic_modelling(data_for_multilabel_classification_clean, 'LDA', 'BOW'), row = 2, col = 2)\n",
    "\n",
    "fig.update_layout(height = 800, width = 1000, title_text = \"Topic Modelling Heatmaps\", title_font = dict(size = 20, family = \"Arial\", weight = \"bold\"), template = \"plotly_white\", \n",
    "                  showlegend = False, coloraxis = dict(colorscale = \"Blues\", colorbar = dict(title=\"Correlation\", tickvals = [-1, 0, 1])))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Aspect**              | **LSA with BOW**                          | **LDA with BOW**                          | **LSA with TFIDF**                        | **LDA with TFIDF**                         |\n",
    "|-------------------------|-------------------------------------------|-------------------------------------------|------------------------------------------|-------------------------------------------|\n",
    "| **Coherence Score**     | 0.4536                                   | 0.4095                                    | 0.4202                                   | 0.3802                                    |\n",
    "| **Top Themes (Topics)** | 1. General food/service (good, food, place)<br>2. Specific dishes (chicken, biryani, fried)<br>3. Veg focus (taste, veg, paneer)<br>4. Social setting (place, friends, chocolate)<br>5. Mixed food mentions (wings, burger, mandi) | 1. General food (chicken, ordered, taste)<br>2. Service and experience (staff, visit, time)<br>3. Delivery-related terms (order, delivery, zomato)<br>4. Ambience and quality (place, great, nice)<br>5. Food mentions (biryani, paneer, quantity) | 1. General satisfaction (good, service, taste)<br>2. Experience focus (place, service, great)<br>3. Social setting (friends, hangout, music)<br>4. Delivery performance (delivery, fast, quick)<br>5. Food issues (biryani, bad, spicy) | 1. General food satisfaction (good, food, taste)<br>2. Service and experience (awesome, nice, staff)<br>3. Delivery speed/issues (fast, superb, waste)<br>4. Food quality (spicy, tasty, avg)<br>5. Specific food items (paratha, stale, dal) |\n",
    "| **Topic Overlap**       | Moderate overlap (Topics 1 and 2)         | Better separation; Topics distinct         | Slight redundancy (Topics 1 and 2 overlap) | Clearer separation, some niche focus       |\n",
    "| **Correlation Highlights** | - Strong negative: Topic 0 and Topic 2 (-0.69)<br>- Weak correlations elsewhere | - Strong negative: Topic 3 and Topic 0 (-0.35)<br>- Topics moderately separated | - Strong negative: Topic 0 and Topic 1 (-0.72)<br>- Weak correlations overall | - Strong negative: Topic 0 and Topic 1 (-0.73)<br>- Weak correlations elsewhere |\n",
    "| **Granularity**         | Balanced, broad topics                   | Granular and specific topics              | Balanced themes with distinct nuances     | Very granular, specific word-level topics  |\n",
    "| **Strengths**           | Best coherence score, broad interpretability | Clear separation of topics               | Captures delivery/service nuances         | Clear separation, niche-specific terms     |\n",
    "| **Weaknesses**          | Topics 1 and 2 overlap                     | Lower coherence, slight noise in terms    | Topics 1 and 2 overlap; slight redundancy   | Lower coherence, noisy rare terms          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#BFD72F' size=5>6. Co-occurrence Analysis</font> <a class=\"anchor\" id=\"P6\"></a>\n",
    "  \n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What dishes are mentioned together in the reviews? Do they form clusters? Can you identify cuisine types based on those clusters?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this phase, we decided to utilize the treated reviews as they were prepared for the multilabel classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering = data_for_multilabel_classification_clean[['Cuisines', 'Lemmas_Treated']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extracted dish mentions from the treated lemmas in the reviews and stored them in a new column, 'Dishes', for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering['Dishes'] = data_clustering['Lemmas_Treated'].apply(find_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We processed this new column to create a structured list of individual dishes. \n",
    "<br>\n",
    "Using this, we generated all possible pairs of co-occurring dishes and computed their frequencies using `Counter`. \n",
    "<br>\n",
    "The unique dishes were extracted and used to construct a square matrix, with rows and columns representing dishes and their co-occurrence counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering['Dishes_List'] = data_clustering['Dishes'].dropna().apply(lambda x: [dish.strip() for dish in x.split(',')])\n",
    "\n",
    "pairs = []\n",
    "for dishes in data_clustering['Dishes_List'].dropna():\n",
    "    pairs.extend(combinations(sorted(dishes), 2))\n",
    "\n",
    "pair_counts = Counter(pairs)\n",
    "\n",
    "unique_dishes = sorted({dish for dishes in data_clustering['Dishes_List'].dropna() for dish in dishes})\n",
    "\n",
    "matrix = pd.DataFrame(0, index = unique_dishes, columns = unique_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Top 10 Dishes Co-occurrence Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the top 10 dishes to simplify the visualization for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for (dish1, dish2), weight in pair_counts.items():\n",
    "    G.add_edge(dish1, dish2, weight = weight)\n",
    "\n",
    "dish_degrees = G.degree(weight = 'weight')\n",
    "top_dishes = sorted(dish_degrees, key = lambda x: x[1], reverse = True)[:10]\n",
    "top_dishes = [dish for dish, _ in top_dishes]\n",
    "\n",
    "subgraph = G.subgraph(top_dishes)\n",
    "\n",
    "sorted_edges = sorted(subgraph.edges(data = True), key = lambda edge: edge[2]['weight'], reverse = True)[:3]\n",
    "top_3_edges = {(u, v): d['weight'] for u, v, d in sorted_edges}\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "pos = nx.spring_layout(subgraph, k = 0.3, seed = 42)  \n",
    "\n",
    "edge_widths = [2 if (u, v) in top_3_edges or (v, u) in top_3_edges else 0.7 for u, v in subgraph.edges]\n",
    "\n",
    "nx.draw_networkx_nodes(subgraph, pos, node_size = 500, node_color = (141/255, 160/255, 203/255), edgecolors = \"black\")\n",
    "\n",
    "nx.draw_networkx_edges(subgraph, pos, alpha = 0.7, width = edge_widths)\n",
    "\n",
    "edge_labels = {edge: weight for edge, weight in top_3_edges.items()}\n",
    "nx.draw_networkx_edge_labels(subgraph, pos, edge_labels = edge_labels, font_size = 10)\n",
    "\n",
    "nx.draw_networkx_labels(subgraph, pos, font_size = 10, font_family = \"sans-serif\")\n",
    "\n",
    "plt.title(f\"Top 10 Dishes Co - occurrence Network\", fontsize = 16, fontweight = \"bold\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The co-occurrence network visualization for the top 10 dishes reveals significant patterns in dish pairings. \n",
    "<br>\n",
    "The **strongest connection**, represented by the thickest edge, is between **Chicken Biryani** and **Chicken** (weight: 174), indicating that these two dishes are frequently mentioned together in reviews. \n",
    "<br>\n",
    "Other notable connections include Chicken and Chicken Tikka (weight: 117) and Butter Chicken and Chicken (weight: 61), highlighting the centrality of chicken-based dishes in customer preferences.\n",
    "\n",
    "From the graph, it is evident that dishes like Chicken Biryani, Chicken, and Butter Chicken act as key nodes, linking with multiple other dishes such as Paneer Tikka, Tandoori Chicken, and Chicken Soup. \n",
    "<br>\n",
    "This suggests that these dishes are central to the dining experience and are often ordered or discussed together.\n",
    "\n",
    "Furthermore, non-chicken dishes such as Gulab Jamun and Butter Naan are also present in the network, showing their relevance as complementary items to chicken-based meals. \n",
    "<br>\n",
    "Overall, the network emphasizes the dominance of chicken-based dishes in co-occurrence and highlights key pairings that could inform menu structuring or promotional strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network Visualization of Dishes for Each Cuisine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cuisines = sorted(set(cuisine.strip() for cuisines in data_clustering['Cuisines'].dropna() for cuisine in cuisines.split(',')))\n",
    "\n",
    "n_rows = -(-len(unique_cuisines) // 3)  \n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize = (20, 5 * n_rows))\n",
    "axes = axes.flatten() \n",
    "\n",
    "for i, cuisine in enumerate(unique_cuisines):\n",
    "    ax = axes[i]\n",
    "    subgraph = get_cuisine_subgraph(cuisine, data_clustering)\n",
    "    pos = nx.spring_layout(subgraph, k = 0.3, seed = 42) \n",
    "\n",
    "    nx.draw_networkx_nodes(subgraph, pos, node_size = 500, node_color = [(141/255, 160/255, 203/255)] * subgraph.number_of_nodes(), edgecolors = \"black\", ax = ax)\n",
    "    nx.draw_networkx_edges(subgraph, pos, alpha = 0.7, ax = ax)\n",
    "    nx.draw_networkx_labels(subgraph, pos, font_size = 8, ax = ax)\n",
    "\n",
    "    ax.set_title(cuisine.capitalize(), fontsize = 10, fontweight = \"bold\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for i in range(len(unique_cuisines), len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network visualizations for different cuisines highlight unique patterns of dish co-occurrence within each cuisine. \n",
    "<br>\n",
    "Prominent dishes are often central in their respective networks, forming strong connections with complementary items. \n",
    "<br>\n",
    "The graphs reveal distinct clusters for each cuisine, showcasing key pairings and relationships that could inform menu optimization, marketing strategies, or customer preference analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we decided to investigate whether the dishes formed distinct clusters based on their co-occurrence in reviews. \n",
    "<br>\n",
    "By applying the `Greedy Modularity` method, we identified communities that represent groups of dishes frequently mentioned together. \n",
    "<br>\n",
    "This clustering approach aimed to uncover underlying patterns and relationships between dishes, providing insights into how they are paired or grouped by customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for (dish1, dish2), weight in pair_counts.items():\n",
    "    G.add_edge(dish1, dish2, weight = weight)\n",
    "\n",
    "communities = list(greedy_modularity_communities(G))\n",
    "\n",
    "cluster_report = []\n",
    "for idx, community in enumerate(communities):\n",
    "    subgraph = G.subgraph(community)\n",
    "    \n",
    "    nodes = list(subgraph.nodes)\n",
    "    size = len(nodes)\n",
    "    degrees = subgraph.degree(weight = 'weight')\n",
    "    prominent_nodes = sorted(degrees, key = lambda x: x[1], reverse = True)[:3]  \n",
    "    prominent_nodes = [node for node, degree in prominent_nodes]\n",
    "    \n",
    "    cluster_report.append({'Cluster ID': idx + 1, 'Nodes': nodes, 'Size': size,})\n",
    "\n",
    "cluster_report = pd.DataFrame(cluster_report)\n",
    "display(cluster_report)\n",
    "\n",
    "colormap = cm.get_cmap('Blues', len(communities))\n",
    "colors = [colormap(i) for i in range(len(communities))]\n",
    "\n",
    "for idx, community in enumerate(communities):\n",
    "    subgraph = G.subgraph(community)\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    \n",
    "    pos = nx.spring_layout(subgraph, k = 0.3, seed = 42) \n",
    "    \n",
    "    nx.draw_networkx_nodes(subgraph, pos, node_size = 500, node_color = colors[idx], edgecolors = \"black\")\n",
    "    \n",
    "    nx.draw_networkx_edges(subgraph, pos, alpha = 0.7)\n",
    "    \n",
    "    nx.draw_networkx_labels(subgraph, pos, font_size = 10, font_family = \"sans-serif\")\n",
    "    \n",
    "    plt.title(f\"Community {idx + 1}\", fontsize = 16, fontweight = \"bold\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering analysis revealed **five distinct communities** of dishes based on their co-occurrence patterns:\n",
    "\n",
    "- **Cluster 1:** The largest cluster, containing 27 dishes, includes a diverse mix such as Shahi Paneer, Barbeque Chicken, and Cheese Burger, indicating these dishes are frequently mentioned together and form a highly interconnected network.\n",
    "- **Cluster 2:** With 22 dishes, this cluster features items like Andhra Chicken Tikka, Apollo Fish, and Afghani Chicken, suggesting a focus on regional and protein-rich dishes.\n",
    "- **Cluster 3:** Comprising 20 dishes, this cluster includes favorites such as Dal Makhani, Paneer Butter Masala, and Achari Chicken, highlighting popular Indian comfort foods often enjoyed together.\n",
    "- **Cluster 4:** A smaller cluster of 6 dishes, including Alfredo Pasta and Creamy Alfredo Pasta, points to a specialization around pasta and creamy dishes, indicating a cohesive group of Italian-inspired options.\n",
    "- **Cluster 5:** The smallest cluster with only 2 dishes, Egg Curry and Barfi Ice Cream, may reflect a niche pairing or a less frequent combination."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
